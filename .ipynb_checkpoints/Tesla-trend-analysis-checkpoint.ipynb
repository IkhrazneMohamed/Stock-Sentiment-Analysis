{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3bf48fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.16.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Visualisation libraries\n",
    "import seaborn as sns\n",
    "\n",
    "import missingno as msno #For missing value visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# From here: https://www.tutorialspoint.com/plotly/plotly_plotting_inline_with_jupyter_notebook.htm\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "# string utility\n",
    "import string\n",
    "\n",
    "# main nlp library and modell\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7536b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import time\n",
    "\n",
    "# Calculation of Performance of Models\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\n",
    "\n",
    "\n",
    "# Modelling Purpose\n",
    "# Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ensemble model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Essentially, Random Forest is a group of decision trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# \n",
    "\n",
    "# Support Vector Classifier, based on SVMs (Support Vector Machines)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Random classification, ignoring inputs\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "## K-nearest neighbor classifieer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af12d427",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 4, saw 10\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## data Exploration\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#ibrahim_data = pd.read_csv(r\"data\\ibrahim.csv\", sep=\";\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#florina_data = pd.read_csv(r\"data\\florina.csv\", sep=\";\")\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#mohamed_data = pd.read_csv(r\"data\\mohamed.csv\", sep=\";\")\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mready_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 4, saw 10\n"
     ]
    }
   ],
   "source": [
    "## data Exploration\n",
    "#ibrahim_data = pd.read_csv(r\"data\\ibrahim.csv\", sep=\";\")\n",
    "#florina_data = pd.read_csv(r\"data\\florina.csv\", sep=\";\")\n",
    "#mohamed_data = pd.read_csv(r\"data\\mohamed.csv\", sep=\";\")\n",
    "\n",
    "dataset = pd.read_csv(r\"data\\ready_data.csv\", sep=\";\")[[\"news\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cd877",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26966c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data cleaning\n",
    "dataset = dataset[[\"news\", \"label\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## null value \n",
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop rows with null label\n",
    "dataset = dataset[dataset['label'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning unecessary text from the string \n",
    "def clean(text):\n",
    "    # cleanup \n",
    "    #text = re.sub('<.*?>+',' ',text) #removing HTML Tags\n",
    "    #text = re.sub('\\n', ' ',text) #removal of new line characters\n",
    "    #text = re.sub(r'\\s+', ' ',text) #removal of multiple spaces\n",
    "    \n",
    "    # tokenize and analyze text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # concatenate tokens that are not stopwords and only alphabethic letters\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    cleaned_text = \" \".join(tokens) if len(tokens)>0 else None\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['news'] = dataset['news'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd056eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb06aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"news\"].isna().value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8819482",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f687278",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"news\"].isna().value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store cleaned data\n",
    "dataset.to_csv(\"data\\cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned data\n",
    "data = pd.read_csv(\"data\\cleaned_data.csv\")\n",
    "print(f\"Rows with empy columns: {data.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c074630",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df = pd.DataFrame({'label':['neutral', 'positiv', 'negativ'],'count':[4009, 1570, 1506]})\n",
    "fig = px.histogram(genre_df,x = 'label',y = 'count',color = 'label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_word_cloud(label,news):\n",
    "    print(label)\n",
    "    wordcloud = WordCloud(width = 400, height = 400, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10).generate(news)\n",
    "    plt.figure(figsize = (7, 7), facecolor = 'white', edgecolor='blue') \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()\n",
    "\n",
    "def make_string(label):\n",
    "    news_str = \"\"\n",
    "    for row_index,row in data[data['label']==label].iterrows():\n",
    "        news_str += \" \" + row['news']\n",
    "    return news_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[\"label\"].unique().tolist()\n",
    "for l in labels:\n",
    "    news_string = make_string(l)\n",
    "    print_word_cloud(l, news_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "## modelling\n",
    "\n",
    "#Converting all the categorical features of 'label' to numerical\n",
    "data['label'] = LabelEncoder().fit_transform(data['label'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CountVectorizer().fit_transform(data['news'])\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [MultinomialNB(),LogisticRegression(),RandomForestClassifier(),SVC(),DummyClassifier(),DecisionTreeClassifier(), KNeighborsClassifier(), SGDClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## without using onevsrest\n",
    "Name = []\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "F1_Score = []\n",
    "Recall = []\n",
    "Time_Taken = []\n",
    "for model in models:\n",
    "    name = type(model).__name__\n",
    "    Name.append(name)\n",
    "    begin = time.time()\n",
    "    model.fit(X_train,y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    end = time.time()\n",
    "    Accuracy.append(accuracy_score(prediction,y_test))\n",
    "    Precision.append(precision_score(prediction,y_test,average = 'macro'))\n",
    "    Recall.append(recall_score(prediction,y_test,average = 'macro'))\n",
    "    F1_Score.append(f1_score(prediction,y_test,average = 'macro'))\n",
    "    Time_Taken.append(end-begin)\n",
    "    print(name + ' Successfully Trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = {'Name':Name,'Accuracy':Accuracy,'Precision_score':Precision,'Recall_score':Precision,\n",
    "        'F1_score':F1_Score,'Time Taken':Time_Taken}\n",
    "model_df = pd.DataFrame(Dict)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.sort_values(by = 'Accuracy',ascending = False,inplace = True)\n",
    "fig = px.line(model_df, x=\"Name\", y=\"Accuracy\", title='Accuracy VS Model')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c07209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.sort_values(by = 'Time Taken',ascending = False,inplace = True)\n",
    "fig = px.line(model_df, x=\"Name\", y=\"Time Taken\", title='Time Taken VS Model')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2fa946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
